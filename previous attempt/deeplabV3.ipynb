{"cells":[{"cell_type":"code","source":["cd 'drive/MyDrive/Colab Notebooks/data'"],"metadata":{"id":"xL2esv7l6RkN","executionInfo":{"status":"ok","timestamp":1689508405809,"user_tz":-540,"elapsed":482,"user":{"displayName":"이재웅","userId":"13807886336430848412"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ef42bb1e-8b10-4536-aa5e-f94bfcdd27a3"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/Colab Notebooks/data'\n","/content/drive/MyDrive/Colab Notebooks/data\n"]}]},{"cell_type":"code","source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"],"metadata":{"id":"Yi4Lr90p1hfF","executionInfo":{"status":"ok","timestamp":1689508406467,"user_tz":-540,"elapsed":6,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# RandomCropDataLoader\n","\n","class SatelliteDatasetRandomCrop(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False, data_set='train', n_crops=20):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","        self.n_crops = n_crops\n","        self.crop_size = (224, 224)\n","        self.data_set = data_set\n","\n","    def random_crop(self, image, mask):\n","        image_patches, mask_patches = [], []\n","\n","        for _ in range(self.n_crops):\n","            crop = A.RandomCrop(height=self.crop_size[0], width=self.crop_size[1])\n","            augmented = crop(image=image, mask=mask)\n","            image_patches.append(augmented['image'])\n","            mask_patches.append(augmented['mask'])\n","\n","        return image_patches, mask_patches\n","\n","    def sliding_window(self, image, stepSize, windowSize, overlap=24):\n","        patch_count = 0\n","        y_start, y_end = 0, windowSize[1]\n","        x_start, x_end = 0, windowSize[0]\n","        for _ in range(5):\n","            for _ in range(5):\n","                yield (x_start, y_start, image[y_start:y_end, x_start:x_end])\n","                x_start += stepSize - overlap\n","                x_end += stepSize - overlap\n","                patch_count += 1\n","            y_start += stepSize - overlap\n","            y_end += stepSize - overlap\n","            x_start, x_end = 0, windowSize[0]\n","        assert patch_count == 25, f\"Patch count should be 25, but got {patch_count} instead\"\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","\n","        if self.data_set == 'train':\n","            patches_image, patches_mask = self.random_crop(image, mask)\n","            for i in range(self.n_crops):\n","                if self.transform:\n","                    augmented = self.transform(image=patches_image[i], mask=patches_mask[i])\n","                    patches_image[i] = augmented['image']\n","                    patches_mask[i] = augmented['mask']\n","        elif self.data_set == 'valid':\n","            patches_image = []\n","            patches_mask = []\n","            for (x, y, window_image) in self.sliding_window(image, stepSize=200, windowSize=(224, 224)):\n","                patches_image.append(window_image)\n","\n","            for (x, y, window_mask) in self.sliding_window(mask, stepSize=200, windowSize=(224, 224)):\n","                patches_mask.append(window_mask)\n","\n","            for i in range(len(patches_image)):\n","                if self.transform:\n","                    augmented = self.transform(image=patches_image[i], mask=patches_mask[i])\n","                    patches_image[i] = augmented['image']\n","                    patches_mask[i] = augmented['mask']\n","\n","\n","\n","        patches_image = np.stack(patches_image, axis=0)  # Stacking the patches\n","        patches_mask = np.stack(patches_mask, axis=0)  # Stacking the patches\n","        return patches_image, patches_mask"],"metadata":{"id":"yakQ-4jt1hcF","executionInfo":{"status":"ok","timestamp":1689508406468,"user_tz":-540,"elapsed":6,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["\n","# traindata 로딩에 사용\n","class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def sliding_window(self, image, stepSize, windowSize, overlap=24):\n","        patch_count = 0\n","        y_start, y_end = 0, windowSize[1]\n","        x_start, x_end = 0, windowSize[0]\n","        for _ in range(5):\n","            for _ in range(5):\n","                yield (x_start, y_start, image[y_start:y_end, x_start:x_end])\n","                x_start += stepSize - overlap\n","                x_end += stepSize - overlap\n","                patch_count += 1\n","            y_start += stepSize - overlap\n","            y_end += stepSize - overlap\n","            x_start, x_end = 0, windowSize[0]\n","        assert patch_count == 25, f\"Patch count should be 25, but got {patch_count} instead\"\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        patches_image = []\n","        patches_mask = []\n","\n","        for (x, y, window_image) in self.sliding_window(image, stepSize=200, windowSize=(224, 224)):\n","            patches_image.append(window_image)\n","\n","        for (x, y, window_mask) in self.sliding_window(mask, stepSize=200, windowSize=(224, 224)):\n","            patches_mask.append(window_mask)\n","\n","        for i in range(len(patches_image)):\n","            if self.transform:\n","                augmented = self.transform(image=patches_image[i], mask=patches_mask[i])\n","                patches_image[i] = augmented['image']\n","                patches_mask[i] = augmented['mask']\n","\n","        patches_image = np.stack(patches_image, axis=0)  # Stacking the patches\n","        patches_mask = np.stack(patches_mask, axis=0)  # Stacking the patches\n","        return patches_image, patches_mask"],"metadata":{"id":"XCD5Ctyd1haC","executionInfo":{"status":"ok","timestamp":1689508406468,"user_tz":-540,"elapsed":6,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# loss function\n","import torch\n","import torch.nn.functional as F\n","\n","class dice_loss(nn.Module):\n","    def __init__(self, smooth=1e-7):\n","        super(dice_loss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, prediction, target):\n","        prediction = torch.sigmoid(prediction)\n","\n","        # Flatten the prediction and target arrays\n","        prediction = prediction.view(-1)\n","        target = target.view(-1)\n","\n","        intersection = (prediction * target).sum()\n","        dice_score = (2. * intersection + self.smooth) / (prediction.sum() + target.sum() + self.smooth)\n","        dice_loss = 1 - dice_score\n","\n","        return dice_loss\n","\n","# Create an instance of the IoU score class\n","class IoUScore(nn.Module):\n","    def __init__(self, smooth=1e-6):\n","        super(IoUScore, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, outputs, targets):\n","        outputs = torch.sigmoid(outputs)\n","\n","        # Binarize the outputs and targets\n","        outputs = (outputs > 0.5).float()\n","        targets = (targets > 0.5).float()\n","\n","        # Flatten the prediction and target tensors\n","        outputs = outputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        # Calculate intersection and union\n","        intersection = (outputs * targets).sum()\n","        union = outputs.sum() + targets.sum() - intersection\n","\n","        iou_score = (intersection + self.smooth) / (union + self.smooth)\n","\n","        return iou_score"],"metadata":{"id":"AaEca_BB1hV3","executionInfo":{"status":"ok","timestamp":1689508406468,"user_tz":-540,"elapsed":6,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["\n","# Define separate transforms for training and validation\n","train_transform = A.Compose(\n","    [\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.5),\n","        A.RandomBrightnessContrast(p=0.5),\n","        A.Rotate(limit=90, p=0.4),\n","        # A.Rotate(limit=90, border_mode=cv2.BORDER_CONSTANT, value=[255, 255, 255], p=0.5),\n","\n","        # A.RandomScale(scale_limit=0.3, p=0.3),\n","        # A.PadIfNeeded(min_height=224, min_width=224),\n","\n","        # color transforms\n","        # A.OneOf(\n","        #     [\n","        #         A.RandomBrightnessContrast(p=1),\n","        #         A.RandomGamma(p=1),\n","        #         A.ChannelShuffle(p=0.2),\n","        #         A.HueSaturationValue(p=1),\n","        #         A.RGBShift(p=1),\n","        #     ], p=0.5),\n","\n","        # noise transforms\n","        A.OneOf([\n","          A.GaussianBlur(p=1),\n","          A.GaussNoise(p=1),\n","        ], p=0.1),\n","\n","        A.Resize(224, 224),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")\n","\n","valid_transform = A.Compose(\n","    [\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")"],"metadata":{"id":"ByMcbgof6oL3","executionInfo":{"status":"ok","timestamp":1689508406469,"user_tz":-540,"elapsed":6,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# from torch.utils.data import random_split\n","\n","# # Create the full dataset\n","# full_dataset = SatelliteDatasetRandomCrop(csv_file='./train.csv', transform=train_transform)\n","\n","# # Split into train and validation datasets\n","# train_size = int(0.9 * len(full_dataset))  # Use % of the data for training\n","# valid_size = len(full_dataset) - train_size  # The rest for validation\n","\n","# train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n","\n","# # Update the transform of the validation dataset\n","# valid_dataset.dataset.transform = valid_transform\n","# valid_dataset.dataset.data_set = 'valid'\n","\n","# # Create dataloaders for train and validation datasets\n","# train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=12)\n","# valid_dataloader = DataLoader(valid_dataset, batch_size=64, shuffle=True, num_workers=12)"],"metadata":{"id":"5FrPwPkN1hT1","executionInfo":{"status":"ok","timestamp":1689508406469,"user_tz":-540,"elapsed":6,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load the full dataset\n","full_data = pd.read_csv('./train.csv')\n","_, valid_data = train_test_split(full_data, test_size=0.3, random_state=42)\n","valid_data.to_csv('./valid_split.csv', index=False)\n","\n","valid_dataset = SatelliteDataset(csv_file='./valid_split.csv', transform=valid_transform)\n","valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=12)\n","\n","# Create dataloaders for train and validation datasets\n","train_dataset = SatelliteDatasetRandomCrop(csv_file='./train.csv', transform=train_transform, n_crops=25)\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=12)\n","\n"],"metadata":{"id":"cbbgxQfzYmtl","executionInfo":{"status":"ok","timestamp":1689508411836,"user_tz":-540,"elapsed":5373,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# ASPP\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class _ASPPModule(nn.Module):\n","    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):\n","        super(_ASPPModule, self).__init__()\n","        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n","                                            stride=1, padding=padding, dilation=dilation, bias=False)\n","        self.bn = BatchNorm(planes)\n","        self.relu = nn.ReLU()\n","\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x = self.atrous_conv(x)\n","        x = self.bn(x)\n","\n","        return self.relu(x)\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class ASPP(nn.Module):\n","    def __init__(self, backbone, output_stride, BatchNorm):\n","        super(ASPP, self).__init__()\n","        if backbone == 'drn':\n","            inplanes = 512\n","        elif backbone == 'mobilenet':\n","            inplanes = 320\n","        else:\n","            inplanes = 2048\n","        if output_stride == 16:\n","            dilations = [1, 6, 12, 18]\n","        elif output_stride == 8:\n","            dilations = [1, 12, 24, 36]\n","        else:\n","            raise NotImplementedError\n","\n","        self.aspp1 = _ASPPModule(inplanes, 256, 1, padding=0, dilation=dilations[0], BatchNorm=BatchNorm)\n","        self.aspp2 = _ASPPModule(inplanes, 256, 3, padding=dilations[1], dilation=dilations[1], BatchNorm=BatchNorm)\n","        self.aspp3 = _ASPPModule(inplanes, 256, 3, padding=dilations[2], dilation=dilations[2], BatchNorm=BatchNorm)\n","        self.aspp4 = _ASPPModule(inplanes, 256, 3, padding=dilations[3], dilation=dilations[3], BatchNorm=BatchNorm)\n","\n","        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n","                                             nn.Conv2d(inplanes, 256, 1, stride=1, bias=False),\n","                                             BatchNorm(256),\n","                                             nn.ReLU())\n","        self.conv1 = nn.Conv2d(1280, 256, 1, bias=False)\n","        self.bn1 = BatchNorm(256)\n","        self.relu = nn.ReLU()\n","        self.dropout = nn.Dropout(0.5)\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x1 = self.aspp1(x)\n","        x2 = self.aspp2(x)\n","        x3 = self.aspp3(x)\n","        x4 = self.aspp4(x)\n","        x5 = self.global_avg_pool(x)\n","        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        return self.dropout(x)\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","\n","def build_aspp(backbone, output_stride, BatchNorm):\n","    return ASPP(backbone, output_stride, BatchNorm)"],"metadata":{"id":"6mwg6FmoMYnD","executionInfo":{"status":"ok","timestamp":1689508411836,"user_tz":-540,"elapsed":23,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# decoder\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Decoder(nn.Module):\n","    def __init__(self, num_classes, backbone, BatchNorm):\n","        super(Decoder, self).__init__()\n","        if backbone == 'resnet' or backbone == 'drn':\n","            low_level_inplanes = 256\n","        elif backbone == 'xception':\n","            low_level_inplanes = 128\n","        elif backbone == 'mobilenet':\n","            low_level_inplanes = 24\n","        else:\n","            raise NotImplementedError\n","\n","        self.conv1 = nn.Conv2d(low_level_inplanes, 48, 1, bias=False)\n","        self.bn1 = BatchNorm(48)\n","        self.relu = nn.ReLU()\n","        self.last_conv = nn.Sequential(nn.Conv2d(304, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","                                       BatchNorm(256),\n","                                       nn.ReLU(),\n","                                       nn.Dropout(0.5),\n","                                       nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n","                                       BatchNorm(256),\n","                                       nn.ReLU(),\n","                                       nn.Dropout(0.1),\n","                                       nn.Conv2d(256, num_classes, kernel_size=1, stride=1))\n","        self._init_weight()\n","\n","\n","    def forward(self, x, low_level_feat):\n","        low_level_feat = self.conv1(low_level_feat)\n","        low_level_feat = self.bn1(low_level_feat)\n","        low_level_feat = self.relu(low_level_feat)\n","\n","        x = F.interpolate(x, size=low_level_feat.size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat((x, low_level_feat), dim=1)\n","        x = self.last_conv(x)\n","\n","        return x\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","def build_decoder(num_classes, backbone, BatchNorm):\n","    return Decoder(num_classes, backbone, BatchNorm)"],"metadata":{"id":"KAD_002AMpLx","executionInfo":{"status":"ok","timestamp":1689508411837,"user_tz":-540,"elapsed":22,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# backbone\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.model_zoo as model_zoo\n","\n","def fixed_padding(inputs, kernel_size, dilation):\n","    kernel_size_effective = kernel_size + (kernel_size - 1) * (dilation - 1)\n","    pad_total = kernel_size_effective - 1\n","    pad_beg = pad_total // 2\n","    pad_end = pad_total - pad_beg\n","    padded_inputs = F.pad(inputs, (pad_beg, pad_end, pad_beg, pad_end))\n","    return padded_inputs\n","\n","\n","class SeparableConv2d(nn.Module):\n","    def __init__(self, inplanes, planes, kernel_size=3, stride=1, dilation=1, bias=False, BatchNorm=None):\n","        super(SeparableConv2d, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, 0, dilation,\n","                               groups=inplanes, bias=bias)\n","        self.bn = BatchNorm(inplanes)\n","        self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n","\n","    def forward(self, x):\n","        x = fixed_padding(x, self.conv1.kernel_size[0], dilation=self.conv1.dilation[0])\n","        x = self.conv1(x)\n","        x = self.bn(x)\n","        x = self.pointwise(x)\n","        return x\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, inplanes, planes, reps, stride=1, dilation=1, BatchNorm=None,\n","                 start_with_relu=True, grow_first=True, is_last=False):\n","        super(Block, self).__init__()\n","\n","        if planes != inplanes or stride != 1:\n","            self.skip = nn.Conv2d(inplanes, planes, 1, stride=stride, bias=False)\n","            self.skipbn = BatchNorm(planes)\n","        else:\n","            self.skip = None\n","\n","        self.relu = nn.ReLU(inplace=True)\n","        rep = []\n","\n","        filters = inplanes\n","        if grow_first:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation, BatchNorm=BatchNorm))\n","            rep.append(BatchNorm(planes))\n","            filters = planes\n","\n","        for i in range(reps - 1):\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(filters, filters, 3, 1, dilation, BatchNorm=BatchNorm))\n","            rep.append(BatchNorm(filters))\n","\n","        if not grow_first:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(inplanes, planes, 3, 1, dilation, BatchNorm=BatchNorm))\n","            rep.append(BatchNorm(planes))\n","\n","        if stride != 1:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(planes, planes, 3, 2, BatchNorm=BatchNorm))\n","            rep.append(BatchNorm(planes))\n","\n","        if stride == 1 and is_last:\n","            rep.append(self.relu)\n","            rep.append(SeparableConv2d(planes, planes, 3, 1, BatchNorm=BatchNorm))\n","            rep.append(BatchNorm(planes))\n","\n","        if not start_with_relu:\n","            rep = rep[1:]\n","\n","        self.rep = nn.Sequential(*rep)\n","\n","    def forward(self, inp):\n","        x = self.rep(inp)\n","\n","        if self.skip is not None:\n","            skip = self.skip(inp)\n","            skip = self.skipbn(skip)\n","        else:\n","            skip = inp\n","\n","        x = x + skip\n","\n","        return x\n","\n","\n","class AlignedXception(nn.Module):\n","    \"\"\"\n","    Modified Alighed Xception\n","    \"\"\"\n","    def __init__(self, output_stride, BatchNorm,\n","                 pretrained=True):\n","        super(AlignedXception, self).__init__()\n","\n","        if output_stride == 16:\n","            entry_block3_stride = 2\n","            middle_block_dilation = 1\n","            exit_block_dilations = (1, 2)\n","        elif output_stride == 8:\n","            entry_block3_stride = 1\n","            middle_block_dilation = 2\n","            exit_block_dilations = (2, 4)\n","        else:\n","            raise NotImplementedError\n","\n","\n","        # Entry flow\n","        self.conv1 = nn.Conv2d(3, 32, 3, stride=2, padding=1, bias=False)\n","        self.bn1 = BatchNorm(32)\n","        self.relu = nn.ReLU(inplace=True)\n","\n","        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1, bias=False)\n","        self.bn2 = BatchNorm(64)\n","\n","        self.block1 = Block(64, 128, reps=2, stride=2, BatchNorm=BatchNorm, start_with_relu=False)\n","        self.block2 = Block(128, 256, reps=2, stride=2, BatchNorm=BatchNorm, start_with_relu=False,\n","                            grow_first=True)\n","        self.block3 = Block(256, 728, reps=2, stride=entry_block3_stride, BatchNorm=BatchNorm,\n","                            start_with_relu=True, grow_first=True, is_last=True)\n","\n","        # Middle flow\n","        self.block4  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block5  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block6  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block7  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block8  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block9  = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block10 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block11 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block12 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block13 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block14 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block15 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block16 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block17 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block18 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","        self.block19 = Block(728, 728, reps=3, stride=1, dilation=middle_block_dilation,\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=True)\n","\n","        # Exit flow\n","        self.block20 = Block(728, 1024, reps=2, stride=1, dilation=exit_block_dilations[0],\n","                             BatchNorm=BatchNorm, start_with_relu=True, grow_first=False, is_last=True)\n","\n","        self.conv3 = SeparableConv2d(1024, 1536, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)\n","        self.bn3 = BatchNorm(1536)\n","\n","        self.conv4 = SeparableConv2d(1536, 1536, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)\n","        self.bn4 = BatchNorm(1536)\n","\n","        self.conv5 = SeparableConv2d(1536, 2048, 3, stride=1, dilation=exit_block_dilations[1], BatchNorm=BatchNorm)\n","        self.bn5 = BatchNorm(2048)\n","\n","        # Init weights\n","        self._init_weight()\n","\n","        # Load pretrained model\n","        if pretrained:\n","            self._load_pretrained_model()\n","\n","    def forward(self, x):\n","        # Entry flow\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","\n","        x = self.block1(x)\n","        # add relu here\n","        x = self.relu(x)\n","        low_level_feat = x\n","        x = self.block2(x)\n","        x = self.block3(x)\n","\n","        # Middle flow\n","        x = self.block4(x)\n","        x = self.block5(x)\n","        x = self.block6(x)\n","        x = self.block7(x)\n","        x = self.block8(x)\n","        x = self.block9(x)\n","        x = self.block10(x)\n","        x = self.block11(x)\n","        x = self.block12(x)\n","        x = self.block13(x)\n","        x = self.block14(x)\n","        x = self.block15(x)\n","        x = self.block16(x)\n","        x = self.block17(x)\n","        x = self.block18(x)\n","        x = self.block19(x)\n","\n","        # Exit flow\n","        x = self.block20(x)\n","        x = self.relu(x)\n","        x = self.conv3(x)\n","        x = self.bn3(x)\n","        x = self.relu(x)\n","\n","        x = self.conv4(x)\n","        x = self.bn4(x)\n","        x = self.relu(x)\n","\n","        x = self.conv5(x)\n","        x = self.bn5(x)\n","        x = self.relu(x)\n","\n","        return x, low_level_feat\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","\n","    def _load_pretrained_model(self):\n","        pretrain_dict = torch.load('./xception_pretrained.pth')\n","        model_dict = {}\n","        state_dict = self.state_dict()\n","\n","        for k, v in pretrain_dict.items():\n","            if k in state_dict:\n","                if 'pointwise' in k:\n","                    v = v.unsqueeze(-1).unsqueeze(-1)\n","                if k.startswith('block11'):\n","                    model_dict[k] = v\n","                    model_dict[k.replace('block11', 'block12')] = v\n","                    model_dict[k.replace('block11', 'block13')] = v\n","                    model_dict[k.replace('block11', 'block14')] = v\n","                    model_dict[k.replace('block11', 'block15')] = v\n","                    model_dict[k.replace('block11', 'block16')] = v\n","                    model_dict[k.replace('block11', 'block17')] = v\n","                    model_dict[k.replace('block11', 'block18')] = v\n","                    model_dict[k.replace('block11', 'block19')] = v\n","                elif k.startswith('block12'):\n","                    model_dict[k.replace('block12', 'block20')] = v\n","                elif k.startswith('bn3'):\n","                    model_dict[k] = v\n","                    model_dict[k.replace('bn3', 'bn4')] = v\n","                elif k.startswith('conv4'):\n","                    model_dict[k.replace('conv4', 'conv5')] = v\n","                elif k.startswith('bn4'):\n","                    model_dict[k.replace('bn4', 'bn5')] = v\n","                else:\n","                    model_dict[k] = v\n","        state_dict.update(model_dict)\n","        self.load_state_dict(state_dict)\n","\n","def build_backbone(backbone, output_stride, BatchNorm):\n","    if backbone == 'xception':\n","        return AlignedXception(output_stride, BatchNorm)\n","    else:\n","        raise NotImplementedError"],"metadata":{"id":"sLK_wqhMMy-e","executionInfo":{"status":"ok","timestamp":1689508411837,"user_tz":-540,"elapsed":21,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class DeepLab(nn.Module):\n","    def __init__(self, backbone='xception', output_stride=16, num_classes=1,\n","                 sync_bn=True, freeze_bn=False):\n","        super(DeepLab, self).__init__()\n","        BatchNorm = nn.BatchNorm2d\n","\n","        self.backbone = build_backbone(backbone, output_stride, BatchNorm)\n","        self.aspp = build_aspp(backbone, output_stride, BatchNorm)\n","        self.decoder = build_decoder(num_classes, backbone, BatchNorm)\n","\n","        self.freeze_bn = freeze_bn\n","\n","    def forward(self, input):\n","        x, low_level_feat = self.backbone(input)\n","        x = self.aspp(x)\n","        x = self.decoder(x, low_level_feat)\n","        x = F.interpolate(x, size=input.size()[2:], mode='bilinear', align_corners=True)\n","        return x\n","\n","    def get_1x_lr_params(self):\n","        modules = [self.backbone]\n","        for i in range(len(modules)):\n","            for m in modules[i].named_modules():\n","                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], nn.BatchNorm2d):\n","                    for p in m[1].parameters():\n","                        if p.requires_grad:\n","                             yield p\n","\n","    def get_10x_lr_params(self):\n","        modules = [self.aspp, self.decoder]\n","        for i in range(len(modules)):\n","            for m in modules[i].named_modules():\n","                if isinstance(m[1], nn.Conv2d) or isinstance(m[1], nn.BatchNorm2d):\n","                    for p in m[1].parameters():\n","                        if p.requires_grad:\n","                             yield p\n"],"metadata":{"id":"5GGnQMIlZvJN","executionInfo":{"status":"ok","timestamp":1689508411837,"user_tz":-540,"elapsed":20,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["\n","# model = UNetWithResnet50Encoder(n_class=1).to(device)\n","model = DeepLab(backbone='xception', num_classes=1).to(device)\n","# model = UNet_3Plus().to(device)\n","weights = torch.load('./0716_DeepLabV3+_model_epoch_22.pth', map_location=device)\n","model.load_state_dict(weights)\n","\n","criterion = dice_loss()\n","init_lr = 0.0001  # 초기 학습률 설정\n","optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n","max_epochs = 50\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","    optimizer, T_0=1, T_mult=2, eta_min=4e-5,\n",")\n","\n","iou_score_calculator = IoUScore().to(device)\n"],"metadata":{"id":"uUuKK_z36wHW","executionInfo":{"status":"ok","timestamp":1689508413524,"user_tz":-540,"elapsed":1707,"user":{"displayName":"이재웅","userId":"13807886336430848412"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["# training loop\n","for epoch in range(max_epochs):  # epoch\n","    model.train()\n","    epoch_loss = 0\n","    epoch_iou_score = 0\n","    num_batches = 0\n","    for images, masks in tqdm(train_dataloader):\n","        num_patches = images.size(1)\n","\n","        batch_loss = 0\n","        batch_iou_score = 0\n","        for i in range(num_patches):\n","            image = images[:, i].float().to(device)\n","            mask = masks[:, i].float().to(device)\n","            # print(mask.unsqueeze(1).shape)\n","            optimizer.zero_grad()\n","            output = model(image)\n","            # print(image.shape)\n","            # print(output.shape)\n","            loss = criterion(output, mask.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_loss += loss.item()\n","            batch_iou_score += iou_score_calculator(output, mask.unsqueeze(1)).item()\n","            # tqdm.write(f'loss_item: {loss.item()}')  # Use tqdm.write instead of print\n","            # sys.stdout.flush()  # Ensure the output is immediately displayed\n","\n","        epoch_loss += batch_loss / num_patches\n","        epoch_iou_score += batch_iou_score / num_patches\n","        num_batches += 1\n","\n","    torch.save(model.state_dict(), f'0716_DeepLabV3+_model_epoch_{epoch+1+22}.pth')\n","    scheduler.step()  # Update learning rate for the next epoch\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/num_batches}, IoU: {epoch_iou_score/num_batches}, Learning rate: {scheduler.get_last_lr()[0]}')\n","\n","    # Validation loop\n","    model.eval()  # set the model to evaluation mode\n","    with torch.no_grad():  # Turn off gradients for validation\n","        val_loss = 0\n","        val_iou_score = 0  # Initialize total IoU score for this epoch\n","        val_num_batches = 0\n","        for val_images, val_masks in tqdm(valid_dataloader):\n","            val_num_patches = val_images.size(1)\n","\n","            val_batch_loss = 0\n","            val_batch_iou_score = 0\n","            for i in range(val_num_patches):\n","                val_image = val_images[:, i].float().to(device)\n","                val_mask = val_masks[:, i].float().to(device)\n","                val_output = model(val_image)\n","\n","                val_loss_item = criterion(val_output, val_mask.unsqueeze(1))\n","\n","                val_batch_loss += val_loss_item.item()\n","                val_batch_iou_score += iou_score_calculator(val_output, val_mask.unsqueeze(1)).item()  # Calculate IoU score for this batch\n","\n","            val_loss += val_batch_loss / val_num_patches\n","            val_iou_score += val_batch_iou_score / val_num_patches\n","\n","            val_num_batches += 1\n","\n","        print(f'Epoch {epoch+1}, Validation Loss: {val_loss/val_num_batches}, Validation IoU: {val_iou_score/val_num_batches}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7grVBTWs7tWS","outputId":"9e0174bc-7b9c-4723-bfb7-fa75e7e4dfd3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 85%|████████▌ | 191/224 [12:19<02:06,  3.83s/it]"]}]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100","mount_file_id":"1ez1kIUyjY16EUZlDUPxdqZPupuk-M27i","authorship_tag":"ABX9TyN51mWJX5hm7JcXLTA2s5bZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}