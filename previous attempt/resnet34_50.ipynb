{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1.12.1\n"]},{"name":"stderr","output_type":"stream","text":["/home/ubin108/anaconda3/envs/jaeung/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","# \n","# torch.cuda.current_device()\n","print(torch.__version__)\n","torch.cuda.is_available()\n","torch.cuda.set_device(1)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The ratio of rows with mask_rle value -1: 0.0\n"]}],"source":["import pandas as pd\n","\n","def calculate_minus_one_ratio(csv_file):\n","    # CSV 파일 읽기\n","    data = pd.read_csv(csv_file)\n","\n","    # mask_rle 값이 -1인 행의 개수 계산\n","    minus_one_count = (data['mask_rle'] == '-1').sum()\n","\n","    # 전체 행의 개수로 나누어 비율 계산\n","    ratio = minus_one_count / len(data)\n","\n","    return ratio\n","\n","csv_file = './0711_RC_model_epoch_79.csv'\n","ratio = calculate_minus_one_ratio(csv_file)\n","print('The ratio of rows with mask_rle value -1:', ratio)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8052,"status":"ok","timestamp":1688813947254,"user":{"displayName":"이재웅","userId":"13807886336430848412"},"user_tz":-540},"id":"G6n05YmJoPtv","outputId":"d8a61900-41de-45aa-b0ca-8848f1312d3d"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","torch.cuda.is_available()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"N0m1cSTmr7Le"},"outputs":[],"source":["# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NQi_YE7ur7G-"},"outputs":[],"source":["# 기존 dataloader - testdata 로딩에 사용\n","class SatelliteDataset1(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FI_L0plLt_FJ"},"outputs":[],"source":["# class SatelliteDataset1(Dataset):\n","#     def __init__(self, csv_file, transform=None, infer=False):\n","#         self.data = pd.read_csv(csv_file)\n","#         self.transform = transform\n","#         self.infer = infer\n","\n","#     def sliding_window(self, image, stepSize, windowSize):\n","#         for y in range(0, image.shape[0], stepSize):\n","#             for x in range(0, image.shape[1], stepSize):\n","#                 yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n","\n","#     def __len__(self):\n","#         return len(self.data)\n","\n","#     def __getitem__(self, idx):\n","#         img_path = self.data.iloc[idx, 1]\n","#         image = cv2.imread(img_path)\n","#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","#         mask_rle = self.data.iloc[idx, 2]\n","#         mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","#         patches_image = []\n","#         patches_mask = []\n","\n","#         for (x, y, window_image) in self.sliding_window(image, stepSize=224, windowSize=(224, 224)):\n","#             if window_image.shape[0] != 224 or window_image.shape[1] != 224:\n","#                 continue\n","#             patches_image.append(window_image)\n","\n","#         for (x, y, window_mask) in self.sliding_window(mask, stepSize=224, windowSize=(224, 224)):\n","#             if window_mask.shape[0] != 224 or window_mask.shape[1] != 224:\n","#                 continue\n","#             patches_mask.append(window_mask)\n","\n","#         for i in range(len(patches_image)):\n","#             if self.transform:\n","#                 augmented = self.transform(image=patches_image[i], mask=patches_mask[i])\n","#                 patches_image[i] = augmented['image']\n","#                 patches_mask[i] = augmented['mask']\n","\n","#         patches_image = np.stack(patches_image, axis=0)  # Stacking the patches\n","#         patches_mask = np.stack(patches_mask, axis=0)  # Stacking the patches\n","#         return patches_image, patches_mask\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# traindata 로딩에 사용(200)\n","class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def sliding_window(self, image, stepSize, windowSize, overlap=24):\n","        patch_count = 0\n","        y_start, y_end = 0, windowSize[1]\n","        x_start, x_end = 0, windowSize[0]\n","        for _ in range(5):\n","            for _ in range(5):\n","                yield (x_start, y_start, image[y_start:y_end, x_start:x_end])\n","                x_start += stepSize - overlap\n","                x_end += stepSize - overlap\n","                patch_count += 1\n","            y_start += stepSize - overlap\n","            y_end += stepSize - overlap\n","            x_start, x_end = 0, windowSize[0]\n","        assert patch_count == 25, f\"Patch count should be 25, but got {patch_count} instead\"\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        patches_image = []\n","        patches_mask = []\n","\n","        for (x, y, window_image) in self.sliding_window(image, stepSize=200, windowSize=(224, 224)):\n","            patches_image.append(window_image)\n","\n","        for (x, y, window_mask) in self.sliding_window(mask, stepSize=200, windowSize=(224, 224)):\n","            patches_mask.append(window_mask)\n","\n","        for i in range(len(patches_image)):\n","            if self.transform:\n","                augmented = self.transform(image=patches_image[i], mask=patches_mask[i])\n","                patches_image[i] = augmented['image']\n","                patches_mask[i] = augmented['mask']\n","\n","        patches_image = np.stack(patches_image, axis=0)  # Stacking the patches\n","        patches_mask = np.stack(patches_mask, axis=0)  # Stacking the patches\n","        return patches_image, patches_mask"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"dkQv4PSi27Zh"},"outputs":[],"source":["# traindata 로딩에 사용(224)\n","\n","class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def sliding_window(self, image, stepSize, windowSize):\n","        for y in range(0, image.shape[0], stepSize):\n","            for x in range(0, image.shape[1], stepSize):\n","                if x + windowSize[0] > image.shape[1] or y + windowSize[1] > image.shape[0]:\n","                    # If remaining pixels are less than window size, start patch from the end of the image\n","                    yield (x, y, image[image.shape[0] - windowSize[1]:, image.shape[1] - windowSize[0]:])\n","                else:\n","                    yield (x, y, image[y:y + windowSize[1], x:x + windowSize[0]])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        patches_image = []\n","        patches_mask = []\n","\n","        for (x, y, window_image) in self.sliding_window(image, stepSize=224, windowSize=(224, 224)):\n","            patches_image.append(window_image)\n","\n","        for (x, y, window_mask) in self.sliding_window(mask, stepSize=224, windowSize=(224, 224)):\n","            patches_mask.append(window_mask)\n","\n","        for i in range(len(patches_image)):\n","            if self.transform:\n","                augmented = self.transform(image=patches_image[i], mask=patches_mask[i])\n","                patches_image[i] = augmented['image']\n","                patches_mask[i] = augmented['mask']\n","\n","        patches_image = np.stack(patches_image, axis=0)  # Stacking the patches\n","        patches_mask = np.stack(patches_mask, axis=0)  # Stacking the patches\n","        return patches_image, patches_mask\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"fvWV4YM8r7Eo"},"outputs":[],"source":["# transform = A.Compose(\n","#     [\n","#         A.Resize(224, 224),\n","#         A.Normalize(),\n","#         ToTensorV2()\n","#     ]\n","# )\n","\n","transform = A.Compose(\n","    [\n","        A.HorizontalFlip(p=0.3),\n","        A.VerticalFlip(p=0.3),\n","        A.Rotate(limit=90, p=0.2),\n","        A.OneOf([\n","                    A.Blur(blur_limit=3, p=0.5),\n","                    A.RandomBrightnessContrast(p=0.5),\n","                ], p=0.2),\n","        A.GaussNoise(p=0.1),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")\n","\n","dataset = SatelliteDataset(csv_file='./train.csv', transform=transform)\n","dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=32)\n","# 1 in dataset[0][1].tolist()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9zkake-4MAA"},"outputs":[],"source":["# UNetWithResnet34Encoder\n","\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision.models import ResNet34_Weights\n","\n","def convrelu(in_channels, out_channels, kernel, padding):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","class UNetWithResnet34Encoder(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        self.base_model = torchvision.models.resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n","        self.base_layers = list(self.base_model.children())\n","\n","        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n","        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n","        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n","        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n","        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n","        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n","        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n","        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n","        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n","        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.conv_up3 = convrelu(256 + 512, 256, 3, 1)\n","        self.conv_up2 = convrelu(128 + 256, 128, 3, 1)\n","        self.conv_up1 = convrelu(64 + 128, 64, 3, 1)\n","        self.conv_up0 = convrelu(64 + 64, 64, 3, 1)\n","\n","        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n","        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n","        self.conv_original_size2 = convrelu(64 + 64, 64, 3, 1)\n","\n","        self.conv_last = nn.Conv2d(64, n_class, 1)\n","\n","    def forward(self, input):\n","        x_original = self.conv_original_size0(input)\n","        x_original = self.conv_original_size1(x_original)\n","\n","        layer0 = self.layer0(input)\n","        layer1 = self.layer1(layer0)\n","        layer2 = self.layer2(layer1)\n","        layer3 = self.layer3(layer2)\n","        layer4 = self.layer4(layer3)\n","\n","        layer4 = self.layer4_1x1(layer4)\n","        x = self.upsample(layer4)\n","        layer3 = self.layer3_1x1(layer3)\n","        x = torch.cat([x, layer3], dim=1)\n","        x = self.conv_up3(x)\n","\n","        x = self.upsample(x)\n","        layer2 = self.layer2_1x1(layer2)\n","        x = torch.cat([x, layer2], dim=1)\n","        x = self.conv_up2(x)\n","\n","        x = self.upsample(x)\n","        layer1 = self.layer1_1x1(layer1)\n","        x = torch.cat([x, layer1], dim=1)\n","        x = self.conv_up1(x)\n","\n","        x = self.upsample(x)\n","        layer0 = self.layer0_1x1(layer0)\n","        x = torch.cat([x, layer0], dim=1)\n","        x = self.conv_up0(x)\n","\n","        x = self.upsample(x)\n","        x = torch.cat([x, x_original], dim=1)\n","        x = self.conv_original_size2(x)\n","\n","        out = self.conv_last(x)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"P-JTJPKROYCb"},"outputs":[],"source":["\n","# UNetWithResnet50Encoder\n","import torch\n","import torch.nn as nn\n","import torchvision\n","from torchvision.models import resnet50, ResNet50_Weights\n","\n","def convrelu(in_channels, out_channels, kernel, padding):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","class UNetWithResnet50Encoder(nn.Module):\n","    def __init__(self, n_class):\n","        super().__init__()\n","\n","        self.base_model = torchvision.models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n","        self.base_layers = list(self.base_model.children())\n","\n","        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n","        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n","        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 256, x.H/4, x.W/4)\n","        self.layer1_1x1 = convrelu(256, 256, 1, 0)\n","        self.layer2 = self.base_layers[5]  # size=(N, 512, x.H/8, x.W/8)\n","        self.layer2_1x1 = convrelu(512, 512, 1, 0)\n","        self.layer3 = self.base_layers[6]  # size=(N, 1024, x.H/16, x.W/16)\n","        self.layer3_1x1 = convrelu(1024, 1024, 1, 0)\n","        self.layer4 = self.base_layers[7]  # size=(N, 2048, x.H/32, x.W/32)\n","        self.layer4_1x1 = convrelu(2048, 2048, 1, 0)\n","\n","        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","\n","        self.conv_up3 = convrelu(1024 + 2048, 1024, 3, 1)\n","        self.conv_up2 = convrelu(512 + 1024, 512, 3, 1)\n","        self.conv_up1 = convrelu(256 + 512, 256, 3, 1)\n","        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n","\n","        self.conv_original_size0 = convrelu(3, 64, 3, 1)\n","        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n","        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n","\n","        self.conv_last = nn.Conv2d(64, n_class, 1)\n","\n","    def forward(self, input):\n","        x_original = self.conv_original_size0(input)\n","        x_original = self.conv_original_size1(x_original)\n","\n","        layer0 = self.layer0(input)\n","        layer1 = self.layer1(layer0)\n","        layer2 = self.layer2(layer1)\n","        layer3 = self.layer3(layer2)\n","        layer4 = self.layer4(layer3)\n","\n","        layer4 = self.layer4_1x1(layer4)\n","        x = self.upsample(layer4)\n","        layer3 = self.layer3_1x1(layer3)\n","        x = torch.cat([x, layer3], dim=1)\n","        x = self.conv_up3(x)\n","\n","        x = self.upsample(x)\n","        layer2 = self.layer2_1x1(layer2)\n","        x = torch.cat([x, layer2], dim=1)\n","        x = self.conv_up2(x)\n","\n","        x = self.upsample(x)\n","        layer1 = self.layer1_1x1(layer1)\n","        x = torch.cat([x, layer1], dim=1)\n","        x = self.conv_up1(x)\n","\n","        x = self.upsample(x)\n","        layer0 = self.layer0_1x1(layer0)\n","        x = torch.cat([x, layer0], dim=1)\n","        x = self.conv_up0(x)\n","\n","        x = self.upsample(x)\n","        x = torch.cat([x, x_original], dim=1)\n","        x = self.conv_original_size2(x)\n","\n","        out = self.conv_last(x)\n","\n","        return out\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"-Aza_ouhE_6c"},"outputs":[],"source":["\n","\n","import torch\n","import torch.nn.functional as F\n","\n","class dice_loss(nn.Module):\n","    def __init__(self, smooth=1e-7):\n","        super(dice_loss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, prediction, target):\n","        prediction = torch.sigmoid(prediction)\n","\n","        # Flatten the prediction and target arrays\n","        prediction = prediction.view(-1)\n","        target = target.view(-1)\n","\n","        intersection = (prediction * target).sum()\n","        dice_score = (2. * intersection + self.smooth) / (prediction.sum() + target.sum() + self.smooth)\n","        dice_loss = 1 - dice_score\n","\n","        return dice_loss\n","\n","class LossFunction(nn.Module):\n","    def __init__(self):\n","        super(LossFunction, self).__init__()\n","        self.bce_with_logits = nn.BCEWithLogitsLoss()\n","\n","    def forward(self, output, target):\n","        # BCE Loss\n","        bce_loss = self.bce_with_logits(output, target)\n","\n","        # Dice Loss\n","        smooth = 1e-7\n","        output_sigmoid = torch.sigmoid(output) \n","        intersect = (output_sigmoid * target).sum()\n","        union = torch.sum(target) + torch.sum(output_sigmoid)\n","        dice_loss = 1 - (2 * intersect + smooth) / (union + smooth)\n","\n","        # Edge Loss\n","        edge_output = F.conv2d(output, torch.Tensor([[[[0, 1, 0], [1, -4, 1], [0, 1, 0]]]]).to(output.device))\n","        edge_target = F.conv2d(target, torch.Tensor([[[[0, 1, 0], [1, -4, 1], [0, 1, 0]]]]).to(target.device))\n","        edge_loss = self.bce_with_logits(edge_output, edge_target)\n","        \n","        # Combine losses\n","        loss = bce_loss + dice_loss + edge_loss\n","\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k4tELtMyr7BH"},"outputs":[],"source":["# # 기본 training\n","\n","# # model 초기화\n","\n","# model = UNetWithResnet50Encoder(n_class=1).to(device)\n","\n","# # loss function과 optimizer 정의\n","# # criterion = torch.nn.BCEWithLogitsLoss()\n","# criterion = dice_loss()\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n","\n","# # training loop\n","# for epoch in range(50):  #50 에폭 동안 학습합니다.\n","#     model.train()\n","#     epoch_loss = 0\n","#     for images, masks in tqdm(dataloader):\n","#         images = images.float().to(device)\n","#         masks = masks.float().to(device)\n","\n","#         optimizer.zero_grad()\n","#         outputs = model(images)\n","#         loss = criterion(outputs, masks.unsqueeze(1))\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         epoch_loss += loss.item()\n","\n","#     print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # UNetWithResnet50Encoder 패치 단위 training\n","# model = UNetWithResnet50Encoder(n_class=1).to(device)\n","# weights = torch.load('model_epoch_52.pth')\n","# model.load_state_dict(weights)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHuVSgJmweFj","outputId":"9d93b84f-e119-44e2-9885-ed0bc9a902c2"},"outputs":[],"source":["# UNetWithResnet50Encoder 패치 단위 training\n","model = UNetWithResnet50Encoder(n_class=1).to(device)\n","# loss function과 optimizer 정의\n","criterion = dice_loss()\n","init_lr = 0.0001  # 초기 학습률 설정\n","optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n","max_epochs = 30\n","# lr_lambda = lambda epoch: (1 - epoch / max_epochs)**0.9\n","# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","    optimizer, T_0=1, T_mult=2, eta_min=2e-5,\n",")\n","\n","# training loop\n","for epoch in range(max_epochs):  # epoch\n","    model.train()\n","    epoch_loss = 0\n","    num_batches = 0\n","    for images, masks in tqdm(dataloader):\n","        num_patches = images.size(1)\n","        batch_loss = 0\n","        for i in range(num_patches):\n","            image = images[:, i].float().to(device)\n","            mask = masks[:, i].float().to(device)\n","\n","            optimizer.zero_grad()\n","            output = model(image)\n","            loss = criterion(output, mask.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","\n","            batch_loss += loss.item()\n","        epoch_loss += batch_loss / num_patches\n","        num_batches += 1\n","\n","    torch.save(model.state_dict(), f'model_epoch_{epoch+53}.pth')\n","    scheduler.step()  # Update learning rate for the next epoch\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/num_batches}, Learning rate: {scheduler.get_last_lr()[0]}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Az_ZYLXO18Ab"},"outputs":[],"source":["# # UNetWithResnet34Encoder 패치 단위 training\n","\n","# model = UNetWithResnet34Encoder(n_class=1).to(device)\n","\n","# # loss function과 optimizer 정의\n","# # criterion = torch.nn.BCEWithLogitsLoss()\n","# criterion = dice_loss()\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)\n","\n","# # training loop\n","# for epoch in range(50):  # 10 에폭 동안 학습합니다.\n","#     model.train()\n","#     epoch_loss = 0\n","#     num_batches = 0\n","#     for images, masks in tqdm(dataloader):\n","#         num_patches = images.size(1)\n","#         batch_loss = 0\n","#         for i in range(num_patches):\n","#             image = images[:, i].float().to(device)\n","#             mask = masks[:, i].float().to(device)\n","\n","#             optimizer.zero_grad()\n","#             output = model(image)\n","#             loss = criterion(output, mask.unsqueeze(1))\n","#             loss.backward()\n","#             optimizer.step()\n","\n","#             batch_loss += loss.item()\n","#         epoch_loss += batch_loss / num_patches\n","#         num_batches += 1\n","\n","#     print(f'Epoch {epoch+1}, Loss: {epoch_loss/num_batches}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"22moL_mn99YQ"},"outputs":[],"source":["transform1 = A.Compose(\n","    [\n","        A.Resize(224, 224),\n","        A.Normalize(),\n","        ToTensorV2()\n","    ]\n",")\n","test_dataset = SatelliteDataset1(csv_file='./test.csv', transform=transform1, infer=True)\n","\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=32)\n","# test_dataset[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KuA1129LVdW0"},"outputs":[],"source":["# # 가중치를 저장\n","# save_path = './Unet_Resnet50_dynlr0.0001_weights.pth'\n","# torch.save(model.state_dict(), save_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfDSaYHLBX3q"},"outputs":[],"source":["# # (저장된 모델이 있다면) 모델 가중치 불러오기\n","# model = UNetWithResnet50Encoder(n_class=1).to(device)\n","# weights = torch.load('./model_epoch_29.pth')\n","# model.load_state_dict(weights)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sN3hSJ1599TE"},"outputs":[],"source":["with torch.no_grad():\n","    model.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","\n","        outputs = model(images)\n","        masks = torch.sigmoid(outputs).cpu().numpy()\n","        masks = np.squeeze(masks, axis=1)\n","        masks = (masks > 0.5).astype(np.uint8) # Threshold\n","\n","        for i in range(len(images)):\n","            mask_rle = rle_encode(masks[i])\n","            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n","                result.append(-1)\n","            else:\n","                result.append(mask_rle)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ud4m4f3g99IX"},"outputs":[],"source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['mask_rle'] = result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Dz25va-8_7g4"},"outputs":[],"source":["submit.to_csv('./submit.csv', index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPeZ1hMVmJV62MyMK2Mrh38","gpuType":"T4","machine_shape":"hm","mount_file_id":"1pDHm3ouHMtix7VUrAsu9reGeIoMRxksP","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"}},"nbformat":4,"nbformat_minor":0}
